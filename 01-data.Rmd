

The first step is important & engineering appropriate dependent ('house prices') and independent variables. Largely, these variables describe:

1. the house's internal & building characteristics;

2. nearby public services & amenities; and,

3. relevant spatial processes & external attributes.

The variables imported & engineered in Section 1 aren't the full list of variables considered in this project. There was a fair amount of background data cleaning, extraction, and ignored variables before settling on this list of 27 variables. In Section 2, that list is narrowed down to 13 after testing independence and correlation. 

## Internal Characteristics

A majority of our variable come from the internal characteristics of the homes as they are the physical property being sold. 

###  Sales & Attributes

The primary data set our team will be using is a collection of Boulder County Single-Family Homes Sales from January 2019 to February 2021 (n = 11,364). 

These points, sales, and building characteristics were curated into a single data set by the Urban Spatial data team and the University of Pennsylvania's Department of Urban Spatial Analytics. However, the data originated from the sales, parcels, and building records of [Boulder County's Tax Assessor Office](https://www.bouldercounty.org/property-and-land/assessor/data-download/). On the local level, the Assessor's Parcel & Building data is the primary, and sometimes *only*, method of recording the population, built environment, and housing transactions. We found it important to heavily utilize this data as it can be found in most local levels in the United States. 

To emphasize, our team and model is utilizing the Assessor's (1) Building Characteristics and (2) Sales properties that were provided to us -- as well as the (3) lot sizes found in the Assessor's Parcels & Land tables. This is an important clarification as our team is developing an model *independent* of Boulder County's Assessed Property Valuation Model and its predicted values. We did **not** pull any other sales or predictions.

```{r vars_sales_import}

# NAD 1983 StatePlane Colorado North FIPS 0501 Feet
col_crs = st_crs('ESRI:102653')

# data from UPenn MUSA
studentData_path =
  "./data/studentData.geojson"
boulder.sales =
  st_read(studentData_path) %>% 
  st_set_crs('ESRI:102254') %>% 
  st_transform(., col_crs) %>%
  mutate(
    toPredict = toPredict %>% sapply(., as.character)
  ) %>%
  rename(id.musa = MUSA_ID)

```

### Lot Size

In order to find the lot sizes of the housing sales, we pre-joined the Home Sales points to the Tax Assessor Parcels through a (1) spatial join and a (1) tabular join on the shared 'address' field. It is important to use lot size in the model as it is a large share of a home's valuation. 

```{r vars_sales_lots}

### ### ### ### ### #
## Original Location of Parcels
###  https://opendata-bouldercounty.hub.arcgis.com/datasets/parcels/explore

## parcel_path =
##   "./data/Boulder_Parcels_20211009.geojson"
## boulder.p.all =
##   st_read(parcel_path) %>%
##   st_transform(col_crs)

### ### ### ### ### ### 
## Original Location of Tax Assessor's Tables 
###  https://www.bouldercounty.org/property-and-land/assessor/data-download/
###  I only used these tables: Account & Parcel Numbers, Owners & Address, Buildings, & Land
###  I DID NOT use any tables with sales or valuation data

boulder.p.join = 
  read.csv("data/Boulder_Sales_Assessor_join.csv")

boulder.sales = 
  merge(
    boulder.sales,
    boulder.p.join,
    on='id.apn'
  ) %>% select(-X)

# To save ~~some~~ *alot* of time and memory space, I pre-joined the Housing Sales and associated Tax Parcels to get the shared account id, ("id.acct"/"strap"), Parcels Numbers ("id.apn"/"parcelno"/"folio"), and lot sizes in square feet.
```

### Engineering Features

The internal variable engineering is primarily creating binary variables or re-configuring categorical/ordinal variables. In one instance, we engineered binary fields if the house had Air Conditioning Systems ("build.ac"), Heating Systems ("build.heatsys"), or both ("build.hvac"). 

``` {r vars_sales_engineer}

boulder.sales =
  boulder.sales %>%
  mutate(
    build.main.sf = mainfloorSF,
    build.stories = 
      ifelse(
        designCodeDscr %in% c('Bi-level', 'MULTI STORY- TOWNHOUSE'),
        2,
        ifelse(
          designCodeDscr %in% c('Split-level'),
          3,
          ifelse(
            designCodeDscr %in% c('2-3 Story'),
            2.5,
            ifelse(
              designCodeDscr %in% c('1 Story - Ranch'),
              1,
              0
            )))),
    build.bsmt.finished = ifelse(
      grepl(' FINISHED', bsmtTypeDscr),
      1,
      0),
    build.garage.covered = ifelse(
      carStorageType %in% c("GRA", "GRB", "GRD", "GRW", "GRF"),
      1,
      0)
    )

## analyzing if the 2-3 stories is larger
boulder.up_down = 
  boulder.sales %>% 
  st_drop_geometry() %>%
  filter(designCodeDscr=='2-3 Story' & build.main.sf>0) %>%
  mutate(
    build.upstairs.sf = TotalFinishedSF - mainfloorSF,
    build.up_down.ratio = round(build.upstairs.sf/build.main.sf,2),
    build.up_down.diff.sf = build.upstairs.sf-build.main.sf,
    build.stories.fix = ifelse(
      (build.up_down.ratio>1)&(build.up_down.diff.sf>100),
      3,
      2
    ))

## boulder.up_down %>%
##   group_by(build.stories.fix) %>%
##   summarise(
##     mean.up_down.ratio = mean(build.up_down.ratio),
##     mean.diff.sf = mean(build.up_down.diff.sf),
##     count = n()
##     ) %>% arrange(count)

boulder.sales[
  (boulder.sales$designCodeDscr=='2-3 Story') & 
  (boulder.sales$build.main.sf>0), 'build.stories'] = 
  boulder.up_down %>% pull(build.stories.fix)

boulder.sales = 
  boulder.sales %>%
  mutate(
    build.quality = qualityCode,
    build.year    = builtYear,
    build.year.adj = EffectiveYear,
    build.garage.sf = carStorageSF,
    build.bsmt.sf   = bsmtSF,
    build.house.sf  = TotalFinishedSF,
    build.garage.sf = carStorageSF,
    build.bedrooms  = nbrBedRoom,
    build.rooms  = nbrRoomsNobath,
    
    build.baths.adj = nbrFullBaths + nbrThreeQtrBaths + nbrHalfBaths*.5,
    build.baths   = nbrFullBaths + nbrThreeQtrBaths + nbrHalfBaths,
    build.living.sf  = build.house.sf + ifelse(
                        build.bsmt.finished==1, 
                        build.bsmt.sf, 0),
    build.ac      = ifelse(Ac %in% c(215,210), 1, 0),
    build.heatsys = ifelse(
      Heating %in% c(810,830,880,850, 820), 1, 0),
    build.hvac    = ifelse(
      (build.heatsys==1)&(build.ac==1),1,0)
    
  )

```

In another instance, the original assessor field of the amount of 'stories' in the building are empty. As a result, we estimated the stories based on (1) the Assessor's Design Codes & (2) narrowing down those ranges of stories based on a ratio of 'upstairs square footage' over 'main floor square footage'. If upstairs square footage has a significantly larger area than the main floor, then it likely has more than 2 floors.

## Public Services

Our model also wants to incorporate external factors, with public services being important. 

###   Boundaries

For administrating & visualizing variables, we import in Boulder County and city boundaries. The main field being use is 'whether or not the home is in an incorporated city'.

``` {r vars_boundaries}

boulder.county = 
  st_read('data/boulder_county.geojson') %>%
  st_sf(., crs=col_crs)

boulder.cities = 
  st_read('data/boulder_cities.geojson') %>%
  st_sf(., crs=col_crs)

boulder.cities.incorp = 
  boulder.cities %>% filter(incorporated=='city') %>%
  mutate(in.city = 1)

boulder.sales = 
  boulder.sales %>%
  st_join(., boulder.cities.incorp %>% select(in.city)) %>%
  mutate(in.city = replace_na(in.city, 0))

```

### Neighborhoods
*Voting Precincts*

In order to spatially categorize the housing sales, we are using 2020 Boulder Voting Precincts as a form of neighborhood. Voting Precincts are apt as they are roughly equally distributed. 

```{r vars_nhoods, warning=FALSE}

boulder.nhoods = 
  st_read('data/boulder_precincts.geojson') %>%
  st_transform(col_crs) %>%
  transmute(nhood_id = DISTRICT %>% as.character())

boulder.sales =
  boulder.sales %>% 
  st_join(
    .,
    boulder.nhoods,
    suffix = c("", ".dupe_join")
  ) %>%
  select(-ends_with(".dupe_join"))

```

The only voting boundaries of concern are those in the core of the City of Boulder. There are 2-3 voting precincts that represent the University of Boulder & nearby commercial areas -- which have little to no housing sales. As a result, a few maps have no predicted sales.

### Elementary Schools
*Ranks & Scores*

The main public service feature our model is elementary school rankings and evaluation scores.The rankings and scores are aggregated by schooldigger.com which aggregates testing, student, and classroom data for all Colorado schools. Then creates scores and ranks.

``` {r vars_elementary_import}

boulder.schools = 
  st_read('data/boulder_county_elementary.geojson') %>% 
  st_transform(col_crs)

```

To match the elementary schools to housing sales, we simply use a nearest neighbor function to match the closest school.

``` {r vars_elementary_nn}

st_geom_ids = function(compare_sf, compare_id){
  compare_geoms = compare_sf[,'geometry']
  compare_ids = compare_sf[[compare_id]]
  if (length(unique(compare_ids)) != length(compare_ids)){
    warning("there are duplicate ids")
    warning(length(compare_ids)-length(unique(compare_ids)))
  }
  rownames(compare_geoms) = compare_ids
  return(compare_geoms)
}

st_nn_id_column = function(
  focus_sf, compare_sf,
  focus_id='', compare_id=''){
  
  #compare_geoms = st_geom_ids(compare_sf, compare_id)
  
  compare_geoms = compare_sf$geometry
  compare_ids = compare_sf[[compare_id]]
  
  if (length(unique(compare_ids)) != length(compare_ids)){
    warning("there are duplicate ids")
    warning(length(compare_ids)-length(unique(compare_ids)))
  }
  
  nn_focus_compare = 
    st_nearest_feature(
      focus_sf,
      compare_geoms,
      check_crs = TRUE
      )
  
  nn_focus_ids = sapply(nn_focus_compare, function(i) compare_ids[i])
  
  return(nn_focus_ids)
}

boulder.sales$ele.nn.id =
  st_nn_id_column(
    boulder.sales, ## 11,364 rows
    boulder.schools, ## 172 cols
    #focus_id = 'id.musa',
    compare_id = 'ele.id'
  ) #%>%
  #str_pad(., 4, pad = "0")

boulder.sales$ele.nn.dist =
  nndist(
    boulder.sales$geometry %>% as.ppp(.), ## 11,364 rows
    boulder.schools$geometry %>% as.ppp(.) ## 172 cols
  )

ele.dist = boulder.sales$ele.nn.dist / mile

boulder.sales = 
  boulder.sales %>%
  merge(
    .,
    boulder.schools %>%
      st_drop_geometry() %>%
      rename(ele.nn.id = ele.id),
    on = 'ele.nn.id',
    all.x = T
  )

## length(boulder.sales$ele.nn.id %>% unique())
## length(boulder.schools$ele.id %>% unique())

ele.group = 
  boulder.sales %>%
  mutate(ele.nn.dist = ele.nn.dist/mile) %>%
  group_by(ele.name) %>% 
  st_drop_geometry() %>%
  summarize(
    count = n(),
    dist.avg = round(mean(ele.nn.dist), 2))

```

The scores and ranks come for 2018-19 and 2020-21 school years. Since our sales are spread across 2019 to 2021, we will simply average the scores and ranks from each school year. 

``` {r vars_elementary_engineer}

boulder.sales = 
  boulder.sales %>%
  mutate(
    ele.score = ((ele.score.2020 + ele.score.2018)/2),
    ele.rank  = ((ele.rank.2018 + ele.rank.2020)/2)
  )

```

## Spatial Processes

Initialy, we had a few more spatial processes (e.g. distance to commercial center, distance to nearby housing units) but they were very process heavy for very insignifigant correlation. 

###  Urbanized Area

Joining the Housing Sales to Colorado Census Urbanized Areas (2010) to create an 'in Urban' binary variables.

``` {r vars_urban}

## https://www.arcgis.com/home/item.html?id=069b5cafe3e34a2585e24ba63cd12b9e
urban = readOGR(dsn = "data/urban.gdb") %>% 
  st_as_sf()
col.urban = urban[grepl(', CO', urban$NAME, fixed = TRUE),] %>%
  st_transform(col_crs) %>%
  mutate(in.urban = 1)

boulder.sales =
  boulder.sales %>% 
  st_join(
    .,
    col.urban %>% select(in.urban),
    suffix = c("", ".dupe_join")
  ) %>%
  select(-ends_with(".dupe_join")) %>%
  mutate(in.urban = replace_na(in.urban, 0))


```

### Census Demographics
*ACS Census Tracts 2019*

We are using ACS Census Tract data as the primary external variables that reflect the larger spatial patterns. The variables we pulled and engineered are Population Density, Housing Unit Density, Median Household Income, and Median Gross Rent.

``` {r vars_acs}

median.HH.income = 83019

boulder.tracts.2019 =
 get_acs(
  year = 2019,
  geography='block group',
  state='08',
  county='013',
  variables=c(
    ## tot pop
    'B01001_001',
    ## median rent
    'B25064_001',
    ## median HH income
    'B19013_001',
    ## House Units
    'B25001_001'
    ),
  geometry=T
  ) %>%
  dplyr::select( -NAME, -moe) %>%
  spread(variable, estimate) %>%
  st_transform(col_crs) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), 0)) %>%
  rename(
    tract.pop   = B01001_001,
    tract.rent  = B25064_001,
    tract.HH.income = B19013_001,
    tract.units = B25001_001
  ) %>%
  mutate(
    tract.area.sqft = geometry %>%
      st_make_valid() %>% st_area() %>%
      as.numeric(),
    tract.area.mile = tract.area.sqft/sqmile,
    tract.units.density = tract.units/tract.area.mile,
    tract.pop.density   = tract.pop/tract.area.mile,
    above.AMI = ifelse(
      tract.HH.income > median.HH.income,
      1,
      0
    )
  )

boulder.sales =
  st_join(
    boulder.sales,
    boulder.tracts.2019
  )

```

## Managing Data

Before setting our data sets off for exploration and modelling, we will organize our variables, remove outliers, and selecting which houses to predict for.

### Organizing Variables

Our variables will be categorized by their role in the model (dependent or independent), housing descriptor categories, and data type. 

``` {r vars_organize}

var.df = 
  data.frame(
    var_name=c('id.musa'),
    var_num=c(-1),
    var_type=c('admin'),
    data_type=c('id')
    )

drop_v = function(bad_var, return_df = var.df){
  return(return_df[!(return_df$var_name %in% c(bad_var)),])
}

add_v = function(vnme, vnum=0, vtype='iv', dtype='', 
                 return_df = var.df,
                 og_df = boulder.sales){
  col_list = c(vnme, vnum, vtype, dtype)
  
  if(!(vnme %in% colnames(og_df))){
    print(vnme)
    return(return_df)
  }
  
  if(vnme %in% return_df$var_name){
    print("already in variables")
    return_df = drop_v(vnme)
  }
  return_df = 
    rbind(return_df,col_list) %>% 
      arrange(desc(var_name)) %>%
      arrange(var_type)
  row.names(return_df) = NULL
  return(return_df)
    }

## sale price
var.df = add_v('price', -1, "dv")

admin_variables = 
  c(
    ## id
    'id.musa', 
    ## to predict housing price
    'toPredict',
    ## address (NA's filled by assessor data)
    'address',
    ## city or county
    'city' #,
    ## voting precinct with local neighborhood names 
    #'nhood_id'
    ## ## neighborhood name of voting precinct
    ## 'neighborhood'
    )

for (col in admin_variables){
  var.df = add_v(col, -2, "admin")
}

var.df = add_v('nhood_id', -2, "admin")

internal_variables = c(
    ###  Year in which the building began construction.
    'build.year',  
    ###  extensive modern remodeling moves the built years to modern  
    'build.year.adj',
    ###  Estimated Stories based on (1) the designCode, & 
    ###  (2) likelihood it is 2 or 3 stories based on ratio of mainfloor sf
    'build.stories',
    ###  Total Bathrooms (fullBA + 3/4BA + 1/2BA)
    'build.baths',
    ###  Total Bathrooms but Adjusted (fullBA + 3/4BA * .75 + 1/2BA * .5)
    'build.baths.adj',
    ###  Number of bedrooms (non-mobile residential only).
    'build.bedrooms',
    ###  Total number of rooms excluding bathrooms (non-mobile residential only). 
    'build.rooms',
    
    ###  Total number of finished square feet in above-grade floors
    'build.house.sf',
    ###  Total Finished Living Space of House & Basement
    'build.living.sf',
    ###  Square footage of the basement (finished & not-finished)
    'build.bsmt.sf',
    ###  Square footage of the finished basement (based on bsmtType)
    'build.bsmt.finished',
    ###  Square footage of the car storage (generally a garage or carport)
    'build.garage.sf',
    ###  TRUE/FALSE a covered garage
    'build.garage.covered',
    
    ###  air conditioning system in the whole house (AcDscr)
    'build.ac',
    ###  heating system in the whole house and not by room (HeatingDscr)
    'build.heatsys',
    ###  both AC & Heating system
    'build.hvac',
    ###  appraiser's building quality code
    'build.quality',
    
    ###  lot square footage 
    'lot.sqft'
)
for (col in internal_variables){
  var.df = add_v(col, 0, "internal")
}

service_variables = c(
    ###  schooldigger.com's aggregate elementary scores & state rankings 
    ###  based on test scores, teacher/student ratio, & school quality 
    ###  AVG of 20-21 & 18-19 
    ###  nearest elementary school to the house
    'ele.score',
    'ele.rank')
for (col in service_variables){
  var.df = add_v(col, 0, "service")
}
spatial_variables = c(
    ###  in an incorporated city
    'in.city',
    ###  in an urbanized area
    'in.urban',
    ###  census tract median gross rent (rent & utilities)
    'tract.rent',
    ###  median household income
    'tract.HH.income',
    ###  housing units per square mile 
    'tract.units.density',
    ###  population per square mile
    'tract.pop.density',
    ###  whether or not the tract is above the median HH income
    'above.AMI'
  )
for (col in spatial_variables){
  var.df = add_v(col, 0, "spatial")
}
```

### Outliers
*Price, House Size, Lot Size, & Rooms*

There appears to be instances of 'outliers' that will skew our results and likely cannot have variables created that account for their prices. The outliers are mostly input errors -- since the assessor's is collecting data from many agencies and stakeholders which leads to inconsistent data inputting. At the same time, there are simply anomalies (e.g. 1 house sold for \$31.5 million when the next highest is \$7.3 million) and potentially administrative deed transfers / house sales for low amounts of money (e.g. a house sold for \$10,000).

```{r vars_outliers, results='hold'}

## For the lot square footage, there are 3 house sales that had properties larger than 229 acres -- an outstanding size. If we run the pearson correlation with those outliers we get .09, but *without* the 3 outliers we get 0.18. A big change in correlation for only 3 observations of the actual 11,364.

boulder.predict.0 = boulder.sales %>%
  filter(.,
         (toPredict == '0'))

price_max_threshold = 31500000
price_instances = nrow(boulder.predict.0 %>% filter(price >= price_max_threshold))
print(glue("Houses that sold for $31.5 million or more: {price_instances}"))
price_min_threshold = 50000
price_min_instances = nrow(boulder.predict.0 %>% filter(price <= price_min_threshold))
print(glue("Houses that sold for $ 25 thousand or less: {price_min_instances}"))
house_min_sf_threshold = 10
sf_instances = nrow(boulder.predict.0 %>% filter(build.house.sf <= house_min_sf_threshold))
print(glue("Houses that have less than {house_min_sf_threshold} square feet: {sf_instances}"))
room_min_threshold = 0
room_instances = nrow(boulder.predict.0 %>% filter(build.rooms <= room_min_threshold))
print(glue("Houses with no rooms: {room_instances}"))
bedroom_min_threshold = 0
bed_instances = nrow(boulder.predict.0 %>% filter(build.bedrooms <= bedroom_min_threshold))
print(glue("Houses with no bedrooms: {bed_instances}"))
bathroom_min_threshold = 0
bathroom_instances = nrow(boulder.predict.0 %>% filter(build.baths.adj == bathroom_min_threshold))
print(glue("Houes with 0 bathrooms: {bathroom_instances}"))
lot_max_acre_threshold = 10
lot_max_sqft_threshold = lot_max_acre_threshold * 43560
lot_instances = nrow(boulder.predict.0 %>% filter(lot.sqft >= lot_max_sqft_threshold))
print(glue("Lots that are larger than {lot_max_acre_threshold} acres: {lot_instances}"))
all_removed_instances = nrow(
  boulder.predict.0 %>% filter(
    (price >= price_max_threshold) |
     (price <= price_min_threshold) |
     (build.house.sf <= house_min_sf_threshold) |
     (build.rooms <= room_min_threshold) |
     (build.baths.adj <= bathroom_min_threshold) |
     (build.bedrooms <= bedroom_min_threshold) |
     (lot.sqft >= lot_max_sqft_threshold) 
  ))
print(glue("Houses with any of the above filters: {all_removed_instances}"))

```

Looking at the filters, we are only removing around 174 housing sales observations out of the total 11,264 (1.5 %). 

### Predicting Dataset

WE split up the data between sales prices known, and the sales prices that are set to be predicted. We will look at the predicted prices for all variables at the end of Section 3.

``` {r vars_partition}

boulder.data = 
  boulder.sales[,
  c(
    var.df$var_name,
    'geometry'
  )] 

var.ivs.all = var.df %>%
  filter(!var_type %in% c('admin','dv')) %>%
  pull(var_name)

vars.admin = var.df %>%
  filter(var_type %in% c('admin')) %>%
  pull(var_name)

var.dv = 'price'

boulder.predict.0 = boulder.data %>%
  filter(.,
         (toPredict == '0') &
         (price < price_max_threshold) &
         (price > price_min_threshold) &
         (build.house.sf > house_min_sf_threshold) &
         (build.rooms > room_min_threshold) &
         (build.baths.adj > bathroom_min_threshold) &
         (build.bedrooms > bedroom_min_threshold) & 
         (lot.sqft < lot_max_sqft_threshold) 
         )

boulder.predict.1 = boulder.data %>%
  filter(., toPredict == '1')

```


