---
title: "Assignment 2: Boulder County House Prices Algorithm"
author: "Alex Nelms"
date: "10/22/2021"
output: 
  bookdown::html_document2: 
    code_folding: hide
    fig_caption: yes
    toc: yes
editor_options: 
  markdown: 
    wrap: sentence
  chunk_output_type: console
---


# 0. Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lubridate)
library(RColorBrewer)
library(patchwork)
library(scales)
library(kableExtra)

library(tidycensus)
library(sf)
library(sp)
library(tmap)
library(ggrepel)
library(tigris)


mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 16,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.text.x = element_text(size = 14))
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 16,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    
    
    plot.background = element_blank(),
    
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    
    strip.text.x = element_text(size = 14)
  )
}


plot_limits = function(
  poly.geometry = '',
  # buffer between plot's limits and the geometry 
  # (in unit of geometry column)
  buffer = 0
){
  # creates bounding box
  poly.bbox =
    poly.geometry %>% st_union() %>%
    # buffers the geometry so the ultimate plot has margins
    st_buffer(buffer) %>%
    st_bbox()
  return(
    # returns the 'coord_sf' function which you can add to any plot
    coord_sf(
      xlim = c(poly.bbox['xmin'], poly.bbox['xmax']),
      ylim = c(poly.bbox['ymin'], poly.bbox['ymax']),
      crs = st_crs(poly.geometry)
  ))}

```

# 1. Data
## 1.A Gathering Data
*Briefly describe your methods for gathering the data.*

### 1.A.1 Import Data
```{r import}

col_crs = st_crs('ESRI:102653')

studentData_path =
  "data/studentData.geojson"
B.sales =
  st_read(studentData_path) %>% 
  st_set_crs('ESRI:102254') %>% 
  st_transform(., col_crs) 

parcel_path = 
  "data/Boulder_Parcels_20211009.geojson" 
B.par = st_read(parcel_path) %>%
  rename(ID = OBJECTID, APN=PARCEL_NO) %>%
  select(ID,APN) %>% 
  st_transform(., col_crs) %>% # North Col State Plane Feet
  mutate(
    geometry = st_make_valid(geometry), 
    area = st_area(geometry)) 
attributes(B.par$area) = NULL
B.par = B.par %>% filter(area>0)


ggplot() + geom_sf(data=B.sales)

address_path = 
  "C:/Users/nelms/Documents/Code/Data/Boulder_AddressPts_20211009.geojson"
B.add = st_read(address_path)%>% 
  st_transform(., col_crs)

acct_path = 
  "C:/Users/nelms/Documents/Code/Data/Account_Parcels.csv"
B.acct = read.csv(acct_path) 

build_path = 
  "C:/Users/nelms/Documents/Code/Data/Buildings.csv"
B.build = read.csv(build_path) 

land_path = 
  "C:/Users/nelms/Documents/Code/Data/Land.csv"
B.land = read.csv(land_path) 

owner_path = 
  "C:/Users/nelms/Documents/Code/Data/Owner_Address.csv"
B.owner = read.csv(owner_path) 

# permits = 
#   "C:/Users/nelms/Documents/Code/Data/Permits.csv"
# B.build = read.csv(build_path) 
```

### 1.A.2 Parcel Joins


```{r parcel}

glimpse(B.par)

```

### 1.A.3 Parcel Clean

```{r parcel_clean}

OG_len = nrow(B.par%>% st_drop_geometry())
APN_len = nrow(B.par %>% st_drop_geometry() %>% distinct(., APN))
ID_len = nrow(B.par %>% st_drop_geometry() %>% distinct(., ID, APN))
area_len = nrow(B.par[B.par$area>0,] %>% st_drop_geometry() )

print(OG_len)
print(OG_len-APN_len)
print(OG_len-ID_len)
print(OG_len-area_len)

B.par[(B.par$area<=0)&(B.par$APN %in% dupe_APN),]

n_occur = 
  data.frame(table(B.par$APN)) %>% 
  rename(APN=Var1) %>% arrange(-Freq)

dupe_APN = n_occur[n_occur$Freq > 1,"APN"]
B.dupe = 
  B.par[B.par$APN %in% dupe_APN,] %>% group_by(APN) %>%
  summarize(geometry = st_union(geometry))

B.par[(B.par$APN == "157505036006")&
        (st_area(B.par$geometry)>0),]



ggplot() + 
  geom_sf(data=B.par, lwd=.1) + 
  geom_sf(data=B.dupe, 
          fill='pink', color='red') + 
  plot_limits(poly.geometry= B.dupe)
```


### 1.A.99 Set Up Price Dataset

```{r price_dataset}

ID_fields = c("MUSA_ID")

variable_fields = c(
# 1. Num of Bedrooms
  "nbrBedRoom",
# 2. Main Square Foot
  "mainfloorSF",
# 3. Full Bathroom
  "nbrFullBaths"
)

B.data = B.sales[,c(ID_fields,variable_fields,'geometry')]

glimpse(B.data)
```

## B.	Table of Summary Statistics
*Present a table of summary statistics with variable descriptions.*

*Sort these variables by their category (internal characteristics, amenities/public services or spatial structure).Check out the `stargazer` package for this.*

Table of summary Statistics 

```{r summary statistics}






```


## C.	Correlation Matrix
*Present a correlation matrix*


```{r correlation matrix}

numericVars <- 
  select_if(st_drop_geometry(.....), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 

```


Plot histograms to assess Normalcy of Data. We don't need to add this in the report, but can be part of data wrangling or feature analysis. ANy that don't appear normal, look at the log transformed 
```{r histograms}

hist(variables, breaks=50)
hist(variables, breaks=50)
.
.
.
hist(variables, breaks=50)




```



```{r log transformed}

dataset$variable <- log(var)
.
.
.

dataset$variable <- Log(1+var)) --> #needed if there are any zeros in the dataset
```

```{r histograms of log transformed}

hist(lnvariables, breaks=50)
hist(lnvariables, breaks=50)
.
.
.
hist(lnvariables, breaks=50)

#if any variables still do not appear normal after log transform, then we will have to deicde if keeping them in or not. Obviously improtant variables (such as dependnet variable house prices) will need to be kep in


```


## D.	Plots of Home Sales Price & 4 Independent Variables
*Present 4 home price correlation scatterplots that you think are of interest.*

*I’m going to look for interesting open data that you’ve integrated with the home sale observations*

Scatterplots of home price correlation that are of interest. Looking at interesting open data that we've intergrated
```{r scatterplots}
plot(dataset$dependentvariable, dataset$independentvariable)
plot(dataset$dependentvariable, dataset$independentvariable)
plot(dataset$dependentvariable, dataset$independentvariable)
plot(dataset$dependentvariable, dataset$independentvariable)

```


#E.	Map of Dependent Variable (Sales Price)
*Develop 1 map of your dependent variable (sale price)*

Map of Dependent Variable - Housing Prices 

```{r map of dependent variable}

ggplot() +
  geom_sf(data = --dataset--, fill = "grey40") +
  geom_sf(data = --boulder--.sf, aes(colour = q5(----)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(----,"----"),
                   name="Quintile\nBreaks") +
  labs(title="-------, Boulder County") +
  mapTheme()

# 3 maps


```


# F.	Maps of 3 Independent Variables
*Develop 3 maps of 3 of your most interesting independent variables.*

# G. Assorted Figures
*Include any other maps/graphs/charts you think might be of interest.*

# 2.	Methods

## 2.A.	Describe Method
*Briefly describe your method (remember who your audience is)*

# 3.	Results 

## A.	Interpret Results
*Briefly interpret each in the context of the Zillow use case*

## B.	Split Training/Test
*Split the ‘toPredict’ == 0 into a separate training and test set using a 75/25 split.*
### Setting up Test and Train datasets


```{r test and train datasets}


inTrain <- createDataPartition(
              y = paste(dataset.sf$Name, dataset.sf$NUM_FLOORS.cat, 
                        dataset.sf$Style, dataset.sf$R_AC), 
              p = .75, list = FALSE)


set.seed(111) #in order to make sure the randomness of the 75/25 split stay the same
boston.training <- dataset.sf[inTrain,] 
boston.test <- dataset.sf[-inTrain,]  
 
```

Run Regression


```{r test and train datasets}


inTrain <- createDataPartition(
              y = paste(dataset.sf$Name, dataset.sf$NUM_FLOORS.cat, 
                        dataset.sf$Style, dataset.sf$R_AC), 
              p = .75, list = FALSE)


set.seed(111) #in order to make sure the randomness of the 75/25 split stay the same
boston.training <- dataset.sf[inTrain,] 
boston.test <- dataset.sf[-inTrain,]  
 
```


## C.	Table of lm Summary (Training)
*Provide a polished table of your (training set) lm summary results (coefficients, R2 etc).*

## D.	Table of MAE & MAPE (Test)
*Provide a polished table of mean absolute error and MAPE for a single test set.*
*Check out the “kable” function for markdown to create nice tables.*

## E.	Cross-Validation Test Results
*Provide the results of your cross-validation tests. This includes mean and standard deviation MAE. Do 100 folds and plot your cross-validation MAE as a histogram.*

*Is your model generalizable to new data?*

## F.	Plot Predicted Prices
*Plot predicted prices as a function of observed prices*

## G.	Map of Residuals (Test)
*Provide a map of your residuals for your test set. Include a Moran’s I test and a plot of the spatial lag in errors.*

## H.	Map of Predicted Values
*Provide a map of your predicted values for where ‘toPredict’ is both 0 and 1.*

## I. Map of MAPE by Neighborhood (Test)
*Using the test set predictions, provide a map of mean absolute percentage error (MAPE) by neighborhood.*

## J.	Plot of MAPE by Neighborhood
*Provide a scatterplot plot of MAPE by neighborhood as a function of mean price by neighborhood.*

## K.	Split City
*Using tidycensus, split your city into two groups (perhaps by race or income) and test your model’s generalizability. Is your model generalizable?*
