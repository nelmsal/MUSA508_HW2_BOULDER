# 3.	Results 

## A.	Interpret Results
*Briefly interpret each in the context of the Zillow use case*

### 3.A.1 Run the Regression

```{r regression_run}

fit <- lm(SalePrice ~ ., data = st_drop_geometry(boulder.predict) %>%
             dplyr::select(SalePrice...........VARIABLES))
```


## 3.B.	Partition Training & Test Sets
*Split the ‘toPredict’ == 0 into a separate training and test set using a 75/25 split.*

### 3.B.1 Partition up Test and Train datasets


```{r partition_train_test}

#need to create a type of neighbors data set i believe, and join it into our pricesKnown data set. 

#need the geometries in order to do the Moran's I test later, and to plot the points 

#need to paste in the variable columns that balances factors for categories acorss the trainign and test sets 

inTrain <- createDataPartition(
              y = paste(boulder.predict$XXXX, boulder.predict$XXXX....), 
              p = .75, list = FALSE)

boulder.training <- boulder.predict[inTrain,] 
boulder.test <- boulder.predict[-inTrain,]  

```


### 3.B.2 Testing & Plotting Regression

```{r regression testing}

boulder.test <-
  boulder.test %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(fit.training, boulder.test), 
         SalePrice.Error = SalePrice.Predict - SalePrice, 
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice), 
         SalePrice.APE = SalePrice.AbsError / SalePrice) %>%   
  filter(SalePrice < 5000000) 

#Mean Error and APE 

mean(boulder.test$SalePrice.AbsError, na.rm = T)#[1] 351280.6
mean(boulder.test$SalePrice.APE, na.rm = T)#[1] 0.7101703
mean(boulder.test$SalePrice.Predict, na.rm = T)


ggplot(data = boulder.test) +
  geom_point(aes(x = SalePrice, y = SalePrice.AbsError)) +
  labs(title = "Figure XX Observed Sale Price and Absolute Error") +
  plotTheme()

ggplot(data = boulder.test) +
  geom_point(aes(x = SalePrice, y = SalePrice.APE)) +
  labs(title = "Figure XX: Observed Sale Price with Absolute Percent Error") +
  plotTheme()

```

#### 3.B.2.A K-Fold Test 
**where should this go?**

Testing for generalization 
Comparing mean average error of K-fold output with our model above that we trained 

```{r k-fold cross-validation}

fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)


fit.cv <- 
  train(SalePrice ~ ., data = st_drop_geometry(boulder.predict) %>% 
                                dplyr::select(SalePrice, XXXXXXXXX), 
     method = "lm", trControl = fitControl, na.action = na.pass)

```

## C.	Table of lm Summary (Training)
*Provide a polished table of your (training set) lm summary results (coefficients, R2 etc).*

```{r regression table}

stargazer(fit.training, type="text", digits=1, title="Table 2: Boulder Trainign Data Regression Output", out = "Training LM.txt")

```

## D.	Table of MAE & MAPE (Test)
*Provide a polished table of mean absolute error and MAPE for a single test set.*
*Check out the “kable” function for markdown to create nice tables.*

## E.	Cross-Validation Test Results
*Provide the results of your cross-validation tests. This includes mean and standard deviation MAE. Do 100 folds and plot your cross-validation MAE as a histogram.*

*Is your model generalizable to new data?*

## F.	Plot Predicted Prices
*Plot predicted prices as a function of observed prices*

## G.	Map of Residuals (Test)
*1. Provide a map of your residuals for your test set*
*2. Include a Moran’s I test*
*3. Plot of the spatial lag in errors.*

### 3.G.1 Map of Test Residuals
```{r Map of test set residuals}

boulder.test$resid <- 
  boulder.test %>%
  as_data_frame() %>%
  add_residuals(., fit.training, var = "resid") %>%
  dplyr::select(resid, Folio) %>%
  pull(resid)


ggplot() +
geom_sf(data = XXXX, fill = "gray90", colour = "XXX") +
    geom_sf(data = XXXX, fill = "XXXX", colour = "XXXX") +
  geom_sf(data = boulder.test, aes(colour = q5(resid))) +
  scale_colour_manual(values = palette5) +
 labs(title = "Figure XXX: Test Set Residual Errors", subtitle = "XXXXX") +
  mapTheme()
```

### 3.G.2 Spatial Lag in Errors 

#### 3.G.2.A Set-Up
```{r Spatial Lag}
library(knitr)
library(kableExtra)
library(scales)

coords <- st_coordinates(boulder.predict)
neighbors <- knn2nb(knearneigh(coords, 5)) #the 5 nearest neighbors

spatialWeights <- nb2listw(neighbors, style="W") 
boulder.predict$lagPrice <- lag.listw(spatialWeights, boulder.predict$SalePrice)

coordinates.test <-  st_coordinates(boulder.test)
neighbors.test <- knn2nb(knearneigh(coordinates.test, 5))
spatialWeights.test <- nb2listw(neighbors.test, style="W")

```
#### 3.G.2.B Plot

```{r Plotting of Spatial Lag }


boulder.test %>%                
  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)) %>%  
  ggplot(aes(lagPriceError, SalePrice)) +
  geom_point() +
  stat_smooth(aes(lagPriceError, SalePrice), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800")+
  labs(title = "Figure XXX: Spatial Lag of Price Errors") +
  plotTheme() + theme(plot.title = element_text(size = 18, colour = "XXXX")) 
```

### 3.G.3 Moran's I

```{r Morans I}

BouldermoranTest <- moran.mc(boulder.test$SalePrice.Error,
                      spatialWeights.test, nsim = 999)


ggplot(as.data.frame(BouldermoranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = BouldermoranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Figure XX: Observed and Permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()
```
## H.	Map of Predicted Values
*Provide a map of your predicted values for where ‘toPredict’ is both 0 and 1.*

```{r map of predicted values}

housespredicted <- b.sales %>%
  mutate(prediction = predict(reg.neighbors, b.sales))

ggplot() +
  geom_sf(data = XXXX, fill = "gray90", colour = "white") +
    geom_sf(data = XXXX, fill = "XXXX", colour = "XXXXX") +
  geom_sf(data = housespredicted, aes(colour = q5(prediction))) +
 scale_colour_manual(values = palette5) +
 labs(title = "Figure XXX: Predicted House Price Values", subtitle = "Boulder County, CO") +
 # facet_wrap(~toPredict) +
  mapTheme()

```

## I. Map of MAPE by neighbors (Test)
*Using the test set predictions, provide a map of mean absolute percentage error (MAPE) by neighbors.*

### I.1.A Accounting for neighbors variance 

```{r neighbors variance into the Regression}

#Make regression model 

reg.neighbors <- lm(SalePrice ~ ., data = as.data.frame(boulder.training) %>% 
                                 dplyr::select(XXXX_neighbors name, SalePrice, XXXXXXXXXXxx))
#Outcomes
boulder.test.neighbors <-
  boulder.test %>%
  mutate(Regression = "neighbors Effects",
         SalePrice.Predict = predict(reg.neighbors, boulder.test), 
         SalePrice.Error = SalePrice - SalePrice.Predict,       
         SalePrice.AbsError = abs(SalePrice - SalePrice.Predict), 
         SalePrice.APE = (abs(SalePrice - SalePrice.Predict)) / SalePrice)%>% 
  filter(SalePrice < 5000000)


#accuracy

bothRegressions <-
  rbind(
    dplyr::select(boulder.test, starts_with("SalePrice"), Regression, neighbors) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
    dplyr::select(boulder.test.nhood, starts_with("SalePrice"), Regression, XXXX_neighbors Name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))   


st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -neighbors) %>%
  filter(Variable == "SalePrice.AbsError" | Variable == "SalePrice.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable(caption = "Table XX: neighbors Effect on Error")


```

### I.1.A Accounting for neighbors variance 

```{r Plotting the predicted prices from the new neighbors variance regression }

bothRegressions %>%
  dplyr::select(SalePrice.Predict, SalePrice, Regression) %>%
    ggplot(aes(SalePrice, SalePrice.Predict)) +
  geom_point() +
  stat_smooth(aes(SalePrice, SalePrice),
             method = "lm", se = FALSE, size = 1, colour="#FA7800") +
  stat_smooth(aes(SalePrice.Predict, SalePrice),
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Figure 10.1: Predicted Sale Price and Observed Price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme() + theme(plot.title = element_text(size = 18, colour = "black"))

```

I.2 Map of MAPE by neighborss

```{r Test Set Predictions, MAPE of neighbors}

names(bothRegressions)[names(bothRegressions) == "OUR NEGHBORHOOD NAME"] <- "Our neighbors values name"


st_drop_geometry(bothRegressions) %>%
  group_by(Regression, neighbors) %>%
  summarise(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
  ungroup() %>%
  left_join(neighbors) %>%
    st_as_sf() %>%
   ggplot() +
    geom_sf(data = XXXX, fill = "XXXX", colour = "XXXX") +
      geom_sf(colour = "gray", aes(fill = q5(mean.MAPE))) +
      scale_fill_manual(values = paletteMap) +
  labs(title = "Figure XXX: MAPE by neighbors") +
      mapTheme()

```

## J.	Plot of MAPE by neighbors
*Provide a scatterplot plot of MAPE by neighbors as a function of mean price by neighbors.*


```{r  scatterplot plot of MAPE by neighbors as a function of mean price by neighbors}

scatter_neighbors <-
    boulder.test.neighbors %>%
    group_by(neighbors) %>%
    dplyr::select(neighbors, SalePrice.APE, SalePrice.Predict)


mean_scatter_neighbors <-
  scatter_neighbors %>%
  group_by(neighbors) %>%
  summarise_at(vars("SalePrice.APE", "SalePrice.Predict"), mean)


plot(mean_scatter_neighbors$SalePrice.Predict, mean_scatter_neighbors$SalePrice.APE, main="Figure XXX: MAPE by neighbors and Mean Price by neighbors", xlab="Mean Price by neighbors", ylab="MAPE by neighbors") +
  plotTheme()


```

## K.	Split City
*Using tidycensus, split your city into two groups (perhaps by race or income)*
*and test your model’s generalizability. Is your model generalizable?*
```{r Testing Model's Generalizability with race or income with Tidycensus}



```

