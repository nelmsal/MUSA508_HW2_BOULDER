# 3. Linear Results 

## A.	Methods



## B.	Partition Training & Test Sets

Before developing the model, this study splits the dataset of known home prices [boulder.predict.0] before training a linear model so we can test that model on unbiased data. Specifically, 75% of the known home prices are randomly split into the training dataset [boulder.train] -- while 25% goes into an unbiased test set [boulder.test].


```{r setup_train_test_partition}



# produces:
#   boulder.train
#   boulder.test

select_v = function(sf, variable_names=c('price', var.ivs)){
  return(sf %>% st_drop_geometry(.) %>%
             select(variable_names))}

select_iv = function(sf, variable_names=var.ivs){
  return(sf %>% st_drop_geometry(.) %>%
             select(variable_names))}

#need the geometries in order to do the Moran's I test later, and to plot the points 

# #need to paste in the variable columns th
# lances factors for categories across the training and test sets. Ken mentions this in Chapter 3 of the textbook

inTrain = createDataPartition(
              y = boulder.predict.0 %>% pull(price), 
              p = .75, list = FALSE)

# inTrain = createDataPartition(
#               y = do.call(paste, c(
#                 boulder.predict.0 %>% select_iv(.)
#                 , sep=" ")), 
#               p = .75, list = FALSE)

boulder.train = 
  boulder.predict.0[inTrain,] %>% 
  select(c(vars.admin,var.dv,var.ivs))

boulder.test = 
  boulder.predict.0[-inTrain,] %>% 
  select(c(vars.admin,var.dv,var.ivs))

```


## C. Train Model 

With a training set, the study can now build its initial training linear model [lm.train] based on the predictor variables constructed in Section 1.

### 1. Fitting a Linear Model

```{r setup_train_lm}

#var.ivs = var.ivs %>% list()
var.dv = var.dv

var.ivs.str = 
  do.call(paste, c(var.ivs %>% list(), collapse = "+"))

fm_equation = as.formula(paste(
  var.dv, var.ivs.str, sep="~"))

print(fm_equation)

##################
lm.train = 
  lm(
    fm_equation, 
    data = boulder.train %>% select_v(.)
    )
```


### 2. Training Summary Table

The results of our model on the training set are presented below. 

``` {r table_train_summ}
tab_num = 3
# stargazer(
#   lm.train, 
#   type="text", digits=1, 
#   title="Table 2: Boulder Training Data Regression Output")

format_nums = function(num_input, digits = 2) ifelse(abs(num_input)>999,
                      count_format(num_input),
                      round_thresh(num_input, digits = digits,int_check = TRUE))

lm.train.summ = 
  lm.train %>%
  tidy() %>%
  transmute(
    Variable = 
      ifelse(term %in% vars.ivs.binary,
             paste(term, '(X = 1)'),
             term
        ), 
    Estimate = format_nums(estimate, digits = 3) %>%
      paste('$', .),
    std.error = format_nums(std.error) %>%
      paste('$', .),
    t.value = format_nums(statistic),
    p.value = p.value %>% round_thresh()
  )

lm.train.summ %>%
  kable(caption = 
          glue('Table {tab_num}: Training Model Summary'),
        align = 'lrrrr') %>%
  kable_styling()
```

The R-Squared value tells us the percentage of our model that accounts for the variance in house sales price, of which it is XXX (indicating hhgow well our model explains the variation), and the low p-value of the f-test, p<0.0001 indicates that our variables are statistically significant, indicating high levels of confidence that the relationship between predictor variables and house sales price is not zero (indicating lack of predicting ability). 

``` {r table_train_err}

tab_num = 4

lm.train %>% glance() %>%
  transmute(
    `RSE` = format_nums(sigma) %>%
      paste('$', .),
    `df ` = format_nums(df.residual),
    `Multiple` = format_nums(r.squared, digits=4),
    `Adjusted` = format_nums(adj.r.squared, digits=4),
    `stat` = format_nums(statistic, digits=1),,
    `df` = format_nums(df),
    p.value = p.value %>% round_thresh()
  ) %>%
  kable(caption = 
          glue('Table {tab_num}: Training Model Fit & Significance Terms'),
        align = 'lrrrr') %>%
  kable_styling() %>%
  add_header_above(
    header = c(
      'Residual Standard Error'=2,
      'R-squared' = 2,
      'F-statistic' = 2,
      ' ' = 1
    ))
```

### 3. Test Errors Table

The error summarized below indicates how well our model is predicting house sales value compared to known house sales values, with the error being that difference (i.e., the residuals). This will tell us how accurate our model is, with an error of XXXX, indicating....XXX

``` {r table_train_error}

tab_num = 5

boulder.test =
  boulder.test %>%
  mutate(
    regression      = "Boulder Test Regression",
    price.predict   = predict(lm.train, .), 
    
    # residuals
    price.error     = price.predict - price,
    price.rmse      = sqrt(mean((price.error)^2)),
    price.abserror  = abs(price.predict - price), 
    price.ape       = price.abserror / price
    ) 

# Mean Error
test.ME = 
  mean(boulder.test$price.error, na.rm = T)

# Mean Absolute Error (MAE)
test.MAE = 
  mean(boulder.test$price.abserror, na.rm = T)

# Mean Absolute Percentage Error (MAPE)
test.MAPE = 
  mean(boulder.test$price.ape, na.rm = T)

# Mean Squared Error (MSE)
test.MSE = 
  mean((boulder.test$price.error)^2)

# Root Mean Squared Error (RMSE)
test.RMSE = 
  boulder.test$price.rmse[1]


# Mean Prediction
test.mean.prediction = 
  mean(boulder.test$price.predict, na.rm = T)

########

data.frame(
  N = nrow(boulder.test) %>% format_nums(),
  `MAE` = test.MAE %>% format_nums()%>%
      paste('$', .),
  `MAPE` = 
    (test.MAPE * 100) %>% round_thresh(
      ., digits = 3, int_check = FALSE) %>%
      paste(., '%'),
  `RMSE` = test.RMSE %>% format_nums()%>%
      paste('$', .)
  )%>%
  kable(caption = 
          glue('Table {tab_num}: Training Model Fit & Significance Terms'),
        align = 'rrrr') %>%
  kable_styling() #%>%
  # add_header_above(
  #   header = c(
  #     'Residual Standard Error'=2,
  #     'R-squared' = 2,
  #     'F-statistic' = 2,
  #     ' ' = 1
  #   ))

```

### 4. Plot Sales Price and Error

```{r plot_train_lm}


fig_num = 5
title = glue('Figure {fig_num}: Test Error & Observed Sales Prices')

grid.arrange(ncol=2,
  ggplot(data = boulder.test) +
    geom_point(aes(x = price, y = price.abserror)) +
    labs(title=title,
      subtitle = "Price & Absolute Error") +
    plotTheme(),
  
  ggplot(data = boulder.test) +
    geom_point(aes(x = price, y = price.ape)) +
    labs(title='', subtitle = "Price & Absolute Percent Error") +
    plotTheme()
)

boulder.predict.0.cv %>% filter(price.ape>5) %>% 
  arrange(price.ape) #%>% select_v()

```

## D.	K-Fold Cross-Validation Model

### 1. Create CV Model with 100 Folds
*Provide the results of your cross-validation tests -- 100 folds.* 
*This includes mean and standard deviation MAE.*

Next, we have to account for generalizability, in order for our model to be effective at predicting unknown, future data. We run a k-fold test on 100 of segments of our data equally split up, and then find the mean average error. 

```{r setup_train_cv}
# k-fold cross-validation 
set.seed(825)


train.control = 
  trainControl(
    method = "cv", 
    number = 100,
    savePredictions = TRUE)

# was train.cv
train.cv =
  train(price ~ .,
    data = boulder.train %>% select_v(.),
    method = "lm", 
    trControl = train.control, na.action = na.pass)


```


### 2. Table of Resampled Cross-Validation

In the table below, we see that for Fold 100, we have an r-Squared value of XXX and a RMSE of XXX (compare it to RMSE and R2 of model). 

``` {r table_train_cv}

tab_num = 6

boulder.predict.0.cv =
  boulder.predict.0 %>%
  select_v() %>%
  mutate(
    regression     = "Final CV Regression",
    price.predict  = predict(train.cv, .), 
    
    # Residual
    price.error    = price.predict - price, 
    price.abserror = abs(price.predict - price), 
    price.ape      = (price.abserror / price),
    
    price.SE = (price.error)^2,
    price.rmse  = sqrt(mean(price.SE))
  )

train.cv.rs = train.cv$resample
train.cv.final = train.cv$finalModel

train.cv.100_final = 
  rbind(
    data.frame(
      Model = c('Training Model'),
      Dataset = c("Testing (25%)"), 
      MAE.mean = test.MAE %>% 
        format_nums() %>% paste('$', .),
      MAE.sd   = '',
      RMSE.mean = test.RMSE %>%
        format_nums() %>% paste('$', .),
      RMSE.sd   = '',
      R2.mean = glance(lm.train)$r.squared %>% 
        format_nums(., digits=4),
      R2.sd   = ''
    ),
    data.frame(
      Model = "100-Folds CV", 
      Dataset = "Training (75%)",
      MAE.mean = train.cv.rs$MAE %>% mean() %>% 
        format_nums() %>% paste('$', .),
      MAE.sd   = train.cv.rs$MAE %>% sd() %>% 
        format_nums() %>% paste('$', .),
      RMSE.mean = train.cv.rs$RMSE %>% mean() %>% 
        format_nums() %>% paste('$', .),
      RMSE.sd   = train.cv.rs$RMSE %>% sd() %>% 
        format_nums() %>% paste('$', .),
      R2.mean = train.cv.rs$Rsquared %>% mean() %>% 
        format_nums(., digits=4),
      R2.sd   = train.cv.rs$Rsquared %>% sd() %>% 
        format_nums(., digits=4)
    ),
    data.frame(
      Model = c('Final CV'),
      Dataset = c("Observed House Sales"), 
      MAE.mean = boulder.predict.0.cv$price.abserror %>% mean() %>% 
        format_nums() %>% paste('$', .),
      MAE.sd   = '',
      RMSE.mean = boulder.predict.0.cv$price.rmse[1] %>%
        format_nums() %>% paste('$', .),
      RMSE.sd   = '',
      R2.mean =  glance(train.cv.final)$r.squared %>% 
        format_nums(., digits=4),
      R2.sd   = ''
    )
  )


train.cv.100_final %>%
  kable(
    caption = glue("Table {tab_num}: Regression Results of One Test Set"),
    col.names = c('Model', 'Dataset', 'Mean', 'SD', 'Mean', 'SD','Mean', 'SD')) %>%
  kable_styling() %>%
  add_header_above(
    header=c(' ' = 2, 'MAE'=2, 'RMSE'=2, 'R-Squared'=2)
  ) %>%
  footnote(alphabet = 
             c(
               "Final Model's Mean's columns are just the single statistic observation"
               ))



```

### 4. Histogram of MAE
*cross-validation MAE as a histogram.*

The histogram below shows the distribution of mean average error across the 100 folds of the K-test. 

```{r histogram_train_cv_MAE}

fig_num = 6

ggplot(train.cv$resample, aes(x=MAE)) +
  geom_histogram() +
  labs(title = glue("Figure {fig_num}: Mean Average Error in Cross Validation Tests")) +
  plotTheme()

```

## E. Model Results
*Is your model generalized to new data?*


### 1. Predict All Home Prices 
*Run Final Model on all Homes in the data set*

```{r setup_all_predict}

boulder.predict = 
  rbind(
    boulder.predict.0[, colnames(boulder.predict.1)],
    boulder.predict.1
  ) %>%
  mutate(
    regression      = "Final Regression",
    price.predict   = predict(train.cv, .)) %>%
  filter(
    price<price_max_threshold
    )
    
```

### 2. Plot Predicted Prices over Observed Prices
*Plot predicted prices as a function of observed prices*

``` {r plot_all_predict_real}


fig_num = 7
title = glue('Figure {fig_num}: Test Error & Observed Sales Prices')

line_min = min(max(boulder.predict$price), max(boulder.predict$price.predict))

ggplot() +
    # geom_line(data=data.frame(
    #     x=c(0,line_min),
    #     y=c(0,line_min)), 
    #     aes(x=x,y =y), 
    #             linetype = "dashed", size=.75, color='grey50') + 
    geom_point(data = boulder.predict, aes(x = price, y = price.predict)) +
    geom_smooth(data = boulder.predict, aes(x = price, y = price.predict), method = lm) + 
    labs(title=title,
      subtitle = "Price & Predicted Price") +
    xlim(min(boulder.predict$price,0), max(boulder.predict$price)) + 
    ylim(min(boulder.predict$price.predict,0), max(boulder.predict$price.predict)) + 
    plotTheme()

```
