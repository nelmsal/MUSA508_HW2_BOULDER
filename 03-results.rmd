# 3. Linear Results 

## A.	Method
*Briefly interpret each in the context of the Zillow use case*

### 1. Interpret Results


## B.	Partition Training & Test Sets

Before developing the model, this study splits the dataset of known home prices [boulder.predict.0] before training a linear model so we can test that model on unbiased data. Specifically, 75% of the known home prices are randomly split into the training dataset [boulder.train] -- while 25% goes into an unbiased test set [boulder.test].


```{r setup_train_test_partition}

# produces:
#   boulder.train
#   boulder.test

select_v = function(sf, variable_names=c('price', boulder.iv)){
  return(sf %>% st_drop_geometry(.) %>%
             select(variable_names))}

select_iv = function(sf, variable_names=boulder.iv){
  return(sf %>% st_drop_geometry(.) %>%
             select(variable_names))}

#need the geometries in order to do the Moran's I test later, and to plot the points 

# #need to paste in the variable columns th
# lances factors for categories across the training and test sets. Ken mentions this in Chapter 3 of the textbook

inTrain = createDataPartition(
              y = do.call(paste, c(
                boulder.predict.0 %>% select_iv(.)
                , sep=" ")), 
              p = .75, list = FALSE)

boulder.train = boulder.predict.0[inTrain,]

boulder.test  = boulder.predict.0[-inTrain,]  


```


## C. Train Model 

With a training set, the study can now build its initial training linear model [lm.train] based on the predictor variables constructed in Section 1.

### 1. Fitting a Linear Model

```{r setup_train_lm}

independent_variables = boulder.iv %>% sort() %>% list()

variables_str = 
  do.call(paste, c(boulder.iv %>% list(), collapse = "+"))

fm_equation = as.formula(paste(
  dependent_variable, variables_str, sep="~"))

print(fm_equation)


##################
lm.train = 
  lm(
    fm_equation, 
    data = boulder.train %>% select_v(.))

boulder.test =
  boulder.test %>%
  mutate(
    regression      = "Boulder Test Regression",
    price.predict   = predict(lm.train, .), 
    
    # residuals
    price.error     = price.predict - price,
    price.rmse      = sqrt(mean((price.error)^2)),
    price.abserror  = abs(price.predict - price), 
    price.ape       = price.abserror / price
    ) 

# Mean Error
test.error = 
  mean(boulder.test$price.error, na.rm = T)

# Mean Absolute Error (MAE)
test.abserror = 
  mean(boulder.test$price.abserror, na.rm = T)

# Mean Absolute Percentage Error (MAPE)
test.ape = 
  mean(boulder.test$price.ape, na.rm = T)

# Mean Prediction
test.predict = 
  mean(boulder.test$price.predict, na.rm = T)

```


### 2. Results

#### A. Summary Table
*Provide a polished table of mean absolute error and MAPE for a single test set.*
*Check out the “kable” function for markdown to create nice tables.*


The results of our model on the training set are presented below. The R-Squared valuu tells us the percentage of our model that accounts for the variance in house sales price, of which it is XXX (indicating hhgow well our model explains the variation), and the low p-value of the f-test, p<0.0001 indicates that our variables are statistically significant, indicating high levels of confidence that the relationship between predictor variables and house sales price is not zero (indicating lack of predicting ability). 

``` {r table_train_lm}

stargazer(
  lm.train, 
  type="text", digits=1, 
  title="Table 2: Boulder Training Data Regression Output", 
  out = "results/Training_LM.txt")

summary(lm.train)
```

#### B. Errors Table

The error summarized below indicates how well our model is predicting hosue sales value compared to known house sales values, with the error being that difference (i.e., the residuals). This will tell us how accurate our model is, with an error of XXXX, indicating....XXX

``` {r table_train_error}



```


#### C. Plot Training Price over Error


```{r plot_train_lm}


library(gridExtra)


grid.arrange(ncol=2,
  ggplot(data = boulder.test) +
    geom_point(aes(x = price, y = price.abserror)) +
    labs(subtitle = "Observed Sale Price and Absolute Error") +
    plotTheme(),
  
  ggplot(data = boulder.test) +
    geom_point(aes(x = price, y = price.ape)) +
    labs(subtitle = "Observed Sale Price with Absolute Percent Error") +
    plotTheme()
)

```



## D.	Test Model

### 1. Create K-Fold Cross-Validation Model
*Provide the results of your cross-validation tests -- 100 folds.* 
*This includes mean and standard deviation MAE.*

Next, we have to account for generalizability, in order for our model to be effective at predicting unknown, future data. We run a k-fold test on 100 of segments of our data equally split up, and then find the mean average error. 

```{r setup_train_cv}
# k-fold cross-validation 
set.seed(825)


train.control = 
  trainControl(
    method = "cv", 
    number = 100,
    savePredictions = TRUE)

# was train.cv
train.cv =
  train(price ~ .,
    data = boulder.train %>% select_v(.),
    method = "lm", 
    trControl = train.control, na.action = na.pass)

train.cv


```


### 3. Run K-Fold Cross-Validation Model
*Using the Un-Observed House Dataset*

```{r predict_train_cv}

# training.results = 
#   data.frame(
#     fm = c()
#     
#   )

boulder.predict.0.cv =
  boulder.predict.0 %>%
  mutate(
    regression     = "Baseline Regression",
    price.predict  = predict(train.cv, .), 
    
    # Residual
    price.error    = price.predict - price, 
    price.abserror = abs(price.predict - price), 
    price.ape      = price.abserror / price
    
    )


fold75 = train.cv$control$indexOut$Resample075

boulder.predict.0.reg75 =
  boulder.predict.0.cv[fold75, c("price", "price.predict")] %>%
  mutate(
    price.error = price.predict - price, 
    price.abserror = abs(price.predict - price), 
    price.ape = price.abserror / price
    ) %>% 
  filter(price < 5000000) 

### ???
train.cv.rs.min = train.cv$resample[75,]
### ???
train.cv.rs.min$MAPE = mean(boulder.predict.0.reg75$price.ape)


round_df = function(x, digits) {
    numeric_columns = sapply(x, mode) == 'numeric'
    x[numeric_columns] =  round(x[numeric_columns], digits)
    x
}

train.cv.rs.min = round_df(train.cv.rs.min, 2)


train.cv.rs.min = train.cv$resample[75,]


```

### 3. Table of Resampled Cross-Validation

In the table below, we see that for Fold075, we have an r-Squared value of XXX and a RMSE of XXX (compare it to RMSE and R2 of model). 

``` {r table_train_cv}

train.cv.rs.min %>%                     
  gather(Variable, Value) %>%
  group_by(Variable) %>%
    spread(Variable, Value) %>%
  kable(caption = "Table XXX: Regression Results of One Test Set") %>%
    kable_classic(full_width = F, html_font = "Cambria")

```

### 4. Histogram of MAE
*cross-validation MAE as a histogram.*

The histogram below shows the distribution of mean average error across the 100 folds of the K-test. 

```{r histogram_train_cv_MAE}

ggplot(train.cv$resample, aes(x=MAE)) +
  geom_histogram() +
  labs(title = "Figure XX: Mean Average Error in Cross Validation Tests") +
  plotTheme()

```




## E. Model Results
*Is your model generalized to new data?*


### 1. Predict All Home Prices 
*Run Final Model on all Homes in the data set*

```{r setup_all_predict}

boulder.predict = 
  boulder.data %>%
  mutate(
    price.predict = 
      predict(train.cv, .)
    )
    
```

### 2. Plot Predicted Prices over Observed Prices
*Plot predicted prices as a function of observed prices*

``` {r plot_all_predict_real}

```
